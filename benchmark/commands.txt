Commands
========

INSTALL CONDA ENVIRONMENT
-------------------------

- Configuration:
	ENV=relish
	PYTHON=3.9
	CUDA_VERSION=11.6
	PYTORCH=1.12

- Create a conda environment:
	conda create -n $ENV python=$PYTHON
	CONDA_ENV_DIR="$(readlink -e "$(dirname "$CONDA_EXE")/../envs/$ENV")"
	mkdir -p "$CONDA_ENV_DIR/etc/conda/activate.d"
	mkdir -p "$CONDA_ENV_DIR/etc/conda/deactivate.d"
	cat << 'EOM' > "$CONDA_ENV_DIR/etc/conda/activate.d/pythonpath.sh"
#!/bin/sh
# The environment name is available under $CONDA_DEFAULT_ENV
if [ -n "$PYTHONPATH" ]; then
	export SUPPRESSED_PYTHONPATH="$PYTHONPATH"
	unset PYTHONPATH
fi
# EOF
EOM
	cat << 'EOM' > "$CONDA_ENV_DIR/etc/conda/deactivate.d/pythonpath.sh"
#!/bin/sh
# The environment name is available under $CONDA_DEFAULT_ENV
if [ -n "$SUPPRESSED_PYTHONPATH" ]; then
	export PYTHONPATH="$SUPPRESSED_PYTHONPATH"
	unset SUPPRESSED_PYTHONPATH
fi
# EOF
EOM
	chmod +x "$CONDA_ENV_DIR/etc/conda/activate.d/pythonpath.sh" "$CONDA_ENV_DIR/etc/conda/deactivate.d/pythonpath.sh"

- Install PyTorch:
	conda activate $ENV
	conda install pytorch=$PYTORCH torchvision cudatoolkit=$CUDA_VERSION -c conda-forge -c pytorch

- Install OpenMM packages:
	conda activate $ENV
	pip install --upgrade openmim
	mim install mmcv-full mmcls mmdet
	mim list
		mmcls      0.23.2     https://github.com/open-mmlab/mmclassification
		mmcv-full  1.6.1      https://github.com/open-mmlab/mmcv
		mmdet      2.25.1     https://github.com/open-mmlab/mmdetection

- Install miscellaneous packages:
	conda activate $ENV
	pip install ptflops wandb
	pip install jupyterlab

- Log into wandb.ai:
	conda activate $ENV
	wandb login

DOCKER
------

- Build docker image:
	cd ~/Code/ReLish/benchmark
	docker buildx build -t relish --load -f docker/Dockerfile .
	docker image ls

- Clean the docker build cache (only dangling vs all):
	docker builder prune [--all]

- Clean old docker images:
	docker image prune

- Nuke relish:
	docker stop relish; docker rm relish; docker image rm relish; docker image prune; docker builder prune --all

- Inspect docker image contents without running it:
	docker create --name=tmp_relish relish:latest
	docker ps -a
	# docker export tmp_relish > /tmp/relish.tar
	mkdir /tmp/relish && docker export tmp_relish | tar -xf - -C /tmp/relish
	rm -rf /tmp/relish
	docker rm tmp_relish

- Run the docker image as a container:
	Ensure wandb API key is available:
		export WANDB_API_KEY=???  # Get from: https://wandb.ai/authorize
		export WANDB_DIR=~/Code/ReLish/benchmark/docker
		export DATASET_PATH=~/Datasets
	Create the container:
		docker create --name relish -it --gpus all --network host --env WANDB_API_KEY --mount "type=bind,source=$DATASET_PATH,target=/Datasets,readonly" --mount "type=bind,source=$WANDB_DIR,target=/relish/log" relish:latest
	Start the container and enter the corresponding bash session interactively:
		docker start -ia relish  # Ctrl+D to exit and stop container, Ctrl+P -> Ctrl+Q to exit without stopping container
	Start the container in the background and attach to it later:
		docker start relish   # If the container was started or run with something other than bash, then this will execute THAT in the background, not bash
		docker attach relish  # Ctrl+D to exit and stop container, Ctrl+P -> Ctrl+Q to exit without stopping container
	Execute something inside the container:
		docker exec -it relish bash
		docker exec -it relish train_cls.py --help
		docker exec -it relish wandb login
		docker exec -it relish wandb agent PROJECT/ENTITY/SWEEP
	Create, start and run a container all in one go:
		docker run --name relish -it --gpus all --network host --env WANDB_API_KEY --mount "type=bind,source=$DATASET_PATH,target=/Datasets,readonly" --mount "type=bind,source=$WANDB_DIR,target=/relish/log" relish:latest
		docker run --rm --name relish -it --gpus all --network host --env WANDB_API_KEY --mount "type=bind,source=$DATASET_PATH,target=/Datasets,readonly" --mount "type=bind,source=$WANDB_DIR,target=/relish/log" relish:latest train_cls.py --help
		docker run --rm --name relish -it --gpus all --network host --env WANDB_API_KEY --mount "type=bind,source=$DATASET_PATH,target=/Datasets,readonly" --mount "type=bind,source=$WANDB_DIR,target=/relish/log" relish:latest wandb agent PROJECT/ENTITY/SWEEP
	Stop and remove the container:
		docker attach relish  # Press Ctrl+D
		docker stop relish
		docker rm relish

- Delete a docker container:
	docker stop relish
	docker rm relish

- Delete a docker image:
	docker image rm relish

CLASSIFICATION DATASETS
-----------------------

- Configuration:
	ENV=relish
	DATASETS=~/Datasets
	MNIST_ROOT="$DATASETS"/MNIST
	FASHION_MNIST_ROOT="$DATASETS"/FashionMNIST
	CIFAR_ROOT="$DATASETS"/CIFAR
	TINY_IMAGENET_ROOT="$DATASETS"/TinyImageNet
	IMAGENETTE_ROOT="$DATASETS"/Imagenette
	IMAGEWOOF_ROOT="$DATASETS"/Imagewoof
	FOOD_ROOT="$DATASETS"/Food101
	IMAGENET1K_ROOT="$DATASETS"/ImageNet1K
	INATURALIST_ROOT="$DATASETS"/iNaturalist

- MNIST:
	mkdir "$MNIST_ROOT"
	wget -P "$MNIST_ROOT/raw" http://yann.lecun.com/exdb/mnist/{train-images-idx3-ubyte.gz,train-labels-idx1-ubyte.gz,t10k-images-idx3-ubyte.gz,t10k-labels-idx1-ubyte.gz}
	for gz in "$MNIST_ROOT/raw"/*.gz; do gunzip -k "$gz"; done

- Fashion MNIST:
	mkdir "$FASHION_MNIST_ROOT"
	wget -P "$FASHION_MNIST_ROOT/raw" http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/{train-images-idx3-ubyte.gz,train-labels-idx1-ubyte.gz,t10k-images-idx3-ubyte.gz,t10k-labels-idx1-ubyte.gz}
	for gz in "$FASHION_MNIST_ROOT/raw"/*.gz; do gunzip -k "$gz"; done

- CIFAR10:
	wget -P "$CIFAR_ROOT" https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
	tar -xf "$CIFAR_ROOT"/cifar-10-python.tar.gz -C "$CIFAR_ROOT" && rm "$CIFAR_ROOT"/cifar-10-python.tar.gz

- CIFAR100:
	wget -P "$CIFAR_ROOT" https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz
	tar -xf "$CIFAR_ROOT"/cifar-100-python.tar.gz -C "$CIFAR_ROOT" && rm "$CIFAR_ROOT"/cifar-100-python.tar.gz
	rm -rf "$CIFAR_ROOT"/cifar-100-python/file.txt~

- Tiny ImageNet:
	wget -P "$TINY_IMAGENET_ROOT" https://image-net.org/data/tiny-imagenet-200.zip
	unzip -q "$TINY_IMAGENET_ROOT"/tiny-imagenet-200.zip -d "$TINY_IMAGENET_ROOT" && rm "$TINY_IMAGENET_ROOT"/tiny-imagenet-200.zip
	rm -r "$TINY_IMAGENET_ROOT"/tiny-imagenet-200/test
	for wnid in "$TINY_IMAGENET_ROOT"/tiny-imagenet-200/train/*; do mv "$wnid"/images/* "$wnid/"; done
	rm -r "$TINY_IMAGENET_ROOT"/tiny-imagenet-200/train/*/images "$TINY_IMAGENET_ROOT"/tiny-imagenet-200/train/*/*.txt
	VALDIR="$TINY_IMAGENET_ROOT"/tiny-imagenet-200/val
	cut -f1,2 < "$VALDIR/val_annotations.txt" | while read file wnid; do mkdir -p "$VALDIR/$wnid"; mv "$VALDIR/images/$file" "$VALDIR/$wnid/"; done
	rm -r "$VALDIR"/{images,val_annotations.txt}

- Imagenette:
	wget -P "$IMAGENETTE_ROOT" https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz
	tar -xf "$IMAGENETTE_ROOT"/imagenette2-320.tgz -C "$IMAGENETTE_ROOT" && rm "$IMAGENETTE_ROOT"/imagenette2-320.tgz

- Imagewoof:
	wget -P "$IMAGEWOOF_ROOT" https://s3.amazonaws.com/fast-ai-imageclas/imagewoof2-320.tgz
	tar -xf "$IMAGEWOOF_ROOT"/imagewoof2-320.tgz -C "$IMAGEWOOF_ROOT" && rm "$IMAGEWOOF_ROOT"/imagewoof2-320.tgz

- Food-101:
	wget -P "$FOOD_ROOT" http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz
	tar -xf "$FOOD_ROOT"/food-101.tar.gz -C "$FOOD_ROOT" && rm "$FOOD_ROOT"/food-101.tar.gz

- ImageNet1K:
	xdg-open https://image-net.org/login.php
		Log in (you need to sign up with an academic email address and wait to be accepted)
		Click 'Download' at the top
		Click on '2012' in the section ILSVRC
		Download 'Training images (Task 1 & 2)' -> Can alternatively just try the wget below
		Download 'Validation images (all tasks)' -> Can alternatively just try the wget below
	mkdir "$IMAGENET1K_ROOT"
	wget -P "$IMAGENET1K_ROOT/ILSVRC-CLS" https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_train.tar
	mkdir "$IMAGENET1K_ROOT/ILSVRC-CLS/val" && tar -xf "$IMAGENET1K_ROOT"/ILSVRC-CLS/ILSVRC2012_img_val.tar -C "$IMAGENET1K_ROOT/ILSVRC-CLS/val" && rm "$IMAGENET1K_ROOT"/ILSVRC-CLS/ILSVRC2012_img_val.tar
	(cd "$IMAGENET1K_ROOT/ILSVRC-CLS/val"; wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash; )
	find "$IMAGENET1K_ROOT/ILSVRC-CLS/val" -name "*.JPEG" | wc -l  # Should be 50000
	mkdir "$IMAGENET1K_ROOT/ILSVRC-CLS/train" && tar -xf "$IMAGENET1K_ROOT"/ILSVRC-CLS/ILSVRC2012_img_train.tar -C "$IMAGENET1K_ROOT/ILSVRC-CLS/train" && rm "$IMAGENET1K_ROOT"/ILSVRC-CLS/ILSVRC2012_img_train.tar
	for class_tar in "$IMAGENET1K_ROOT/ILSVRC-CLS/train"/*.tar; do class_dir="${class_tar%.tar}"; mkdir "$class_dir" && tar -xf "$class_tar" -C "$class_dir" && rm "$class_tar"; done
	find "$IMAGENET1K_ROOT/ILSVRC-CLS/train" -name "*.JPEG" | wc -l  # Should be 1281167

- iNaturalist:
	conda activate $ENV
	python -c "import torchvision.datasets"$'\n'"[torchvision.datasets.INaturalist('$INATURALIST_ROOT', version, download=True) for version in ('2021_train', '2021_train_mini', '2021_valid')]"
	rm -f "$INATURALIST_ROOT"/2021_{train,train_mini,valid}.tgz

TRAIN CLASSIFICATION MODEL
--------------------------

- Configuration:
	ENV=relish
	RELISH_DIR=~/Code/ReLish
	export DATASET_PATH=~/Datasets
	# export CUDA_VISIBLE_DEVICES=1

- Cache the relevant datasets in RAM:
	mkdir /dev/shm/Datasets && cp -r ~/Datasets/{MNIST,FashionMNIST,CIFAR,TinyImageNet,Imagenette,Imagewoof} /dev/shm/Datasets
	ls -lAh /dev/shm/Datasets
	export DATASET_PATH=/dev/shm/Datasets
	# RUN TRAINING SCRIPT
	# UNDO AFTER TRAINING: rm -rf /dev/shm/Datasets

- Train single classification model:
	cd "$RELISH_DIR"/benchmark
	conda activate $ENV
	./train_cls.py --dataset MNIST --model fcnet --epochs 50 --batch_size 64 --amp
	./train_cls.py --dataset FashionMNIST --model fcnet --epochs 50 --batch_size 64 --amp
	./train_cls.py --dataset CIFAR10 --model wide2_resnet14_g3 --epochs 80 --batch_size 64 --amp
	./train_cls.py --dataset CIFAR100 --model resnet34-4 --epochs 120 --batch_size 64
	./train_cls.py --dataset TinyImageNet --model resnet101-8 --epochs 120 --batch_size 64
	./train_cls.py --dataset Imagenette --model efficientnet_v2_s --epochs 120 --batch_size 32
	./train_cls.py --dataset Imagewoof --model resnext50_32x4d --epochs 300 --batch_size 32 --lr_scale 3
	./train_cls.py --dataset Food101 --model swin_t --epochs 120 --warmup_epochs 20 --batch_size 32 --optimizer adamw
	./train_cls.py --dataset ImageNet1K --model swin_t --epochs 80 --batch_size 32
	./train_cls.py --dataset iNaturalist --model efficientnet_v2_s --epochs 80 --batch_size 32

- Train classification model sweep:
	cd "$RELISH_DIR"/benchmark
	conda activate $ENV
	export WANDB_DIR="$RELISH_DIR"/benchmark/log
	wandb sweep sweep/cls_SWEEPNAME.yaml
	<WANDB AGENT COMMAND FROM ABOVE OUTPUT>  # [--count NUM]

- Calculate result statistics:
	cd "$RELISH_DIR"/benchmark
	conda activate $ENV
	results/mean_stats_table.py --project ReLish --any_tags cifar10_act_func --metric valid_top1_max --group_by act_func

- Train all classification model sweeps (excluding ImageNet1K):
	cd "$RELISH_DIR"/benchmark
	conda activate $ENV
	export WANDB_DIR="$RELISH_DIR"/benchmark/log
	for sweep in sweep/cls_{mnist,cifar10,cifar100,tinyimagenet,imagenette,imagewoof,food101}_model.yaml; do wandb sweep "$sweep"; done
	# --> Run all the wandb agent commands from the above
	# --> Select a nominal model for each dataset and update all the dependent sweep files
	for sweep in sweep/cls_{mnist,cifar10,cifar100,tinyimagenet,imagenette,imagewoof,food101}_act_func.yaml; do wandb sweep "$sweep"; done
	# --> Run all the wandb agent commands from the above
	# --> Select a collection of activation functions that are all high performing and update all the dependent sweep files
	for sweep in sweep/cls_{mnist,cifar10,cifar100,tinyimagenet,imagenette,imagewoof,food101}_act_func_*.yaml; do wandb sweep "$sweep"; done
	# --> Run all the wandb agent commands from the above

- Create plots for each dataset:
	Create results section:
		Go to project page
		'Add a section' at bottom
		Drag to top spot
		Rename via '...' to 'Results'
	Plot model:
		Add panel -> Custom chart -> Scatter plot (nominal x, field sorted)
		Remove limit with X
		Add filters with ... -> filters -> Dropdown
			Filter by State = finished, Sweep != null, and then every single config that can possibly make a difference (including Dataset)
		Summary variables: params, valid_top1_max
		Config variables: act_func, model
		Chart fields: groupKey = act_func, x = model, y = valid_top1_max, order = params, title = {DATASET} model E{EPOCHS} B{BATCH_SIZE} (you can check the sweep yaml if in doubt, don't reopen the filters)
		OK -> Duplicate panel -> Change valid_top1_max to train_top1

- Running in tmux:
	tmux new -s relish
		Ctrl+B,"
		Pane 0:
			SET UP AS YOU NORMALLY WOULD
			gitpull
			START YOUR RUN / SWEEP (Middle click doesn't work => Use Ctrl+Shift+V or Ctrl+B,] if pasting from within tmux via mouse select)
		Pane 1:
			watch nvidia-smi
			Ctrl+C
		Ctrl+B,D
	tmux attach -d -t relish

CUSTOM WANDB CHARTS
-------------------

- Add a section for custom charts:
	Go to wandb main project page of concern
	Scroll all the way down and click 'Add a section'
	Click on the ellipsis at the top right of the new section and rename the section to 'Custom Charts' or whatever is desired

- Adding a custom chart:
	In the custom charts section click 'Add Panel -> Custom chart'
	Select the type of plot you want on the top left (start with 'Scatter plot' if you're not sure)
	Update the data query:
		Click the appearing X above 'limit:' to make sure all data you are looking for will be plotted (just be careful in selecting your data now though)
		Click the ellipsis and add 'filters:'
			Click the down arrow on '# filters' and configure as many filters as appropriate
			Note that all data in the project (including all data from all sweeps and individual runs) is queried, so best be specific/explicit about all possible configs
			Filters: State == finished, Sweep (explicit sweep, or != null), All configs that could possibly affect the output y-value (even if there is currently only one choice)
			Note at time of writing there is a bug that you cannot expand the filters again once you've had them open (only choice is to completely delete the filters with the X and do them all again OR edit the filters in the Vega-Lite editor, that seems to be more stable)
		Select the keys (that you're interested in from the various categories available (use the drop down arrows, plus buttons, and click between the quotation marks)
		Categories:
			config: Single values corresponding to run config parameters
			summary: Single final logged values of a run, i.e. as presented in the run table
			history*: Series of logged data for a particular run
	Select the available chart fields from the keys you just selected, and hopefully you can see a suitable preview of the chart
	Use groupKeys if you don't want each run to be a separate data series, but instead for example all runs with a particular config parameter value

- Customise the definition of a scatter plot -> Click 'Edit' next to 'Scatter plot':
	If the x-axis data is not quantitative, then change encoding.x.type to 'nominal' (hover for more options and explanations):
		"encoding": {
			"x": {"field": "${field:x}", "type": "nominal"},
	If you don't need 0 to be shown on an axis, e.g. the y-axis then add encoding.y.scale.zero = false:
		"encoding": {
			"y": {"field": "groupedY", "type": "quantitative", "axis": {"title": "${field:y}"}, "scale": {"zero": false}},

- For totally custom Vega-Lite plots refer to the 'vega' directory
